{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jDPJ5f0meQZ"
      },
      "outputs": [],
      "source": [
        "!pip install pysbd\n",
        "!pip install sentence-transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "-_FXy1o2yWjX"
      },
      "source": [
        "pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRAPAxaAmNC0",
        "outputId": "67cdae45-435e-42e3-8af6-25a41183b467"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\users\\ameya\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Ameya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "from collections import Counter\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "import pysbd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import words\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from datasets import load_dataset\n",
        "from datasets import load_metric\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "import evaluate\n",
        "import bert_score\n",
        "\n",
        "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
        "tqdm.pandas()\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9ZIOVnYnlQu"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/682-project/data/metadata.tsv'\n",
        "\n",
        "# Import Pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Read the TSV file as a DataFrame\n",
        "metadata_training = pd.read_csv(file_path, sep='\\t')\n",
        "print(\"Columns: \", metadata_training.columns)\n",
        "print(\"Shape: \", metadata_training.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG-SO6qEoTPK"
      },
      "outputs": [],
      "source": [
        "metadata_training.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s5v8fN6oaKx"
      },
      "outputs": [],
      "source": [
        "print(\"Episode Duration Stats:\\n\"\n",
        "      f\"{metadata_training['duration'].describe()}\")\n",
        "metadata_training['duration'].hist(bins=1000, figsize=(10,5), log=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0uZ9pv6odmN"
      },
      "outputs": [],
      "source": [
        "show_episodes = metadata_training.groupby(['show_filename_prefix']).apply(lambda x: list(zip(x['episode_filename_prefix'], x['episode_description']))).to_dict()\n",
        "show_n_episodes = {k: len(v) for k, v in show_episodes.items()}\n",
        "print(\"Statistics about number of episodes per show:\\n\"\n",
        "      f\"{pd.Series(show_n_episodes.values()).describe()}\")\n",
        "pd.Series(show_n_episodes.values()).hist(bins=1000, figsize=(10,5), log=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2jtIjNdooYM"
      },
      "source": [
        "#### Removing NAN values from the episode_description and show_description columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpqnheNuog0y"
      },
      "outputs": [],
      "source": [
        "print(\"Before dropping NaN values: \\n\", metadata_training.isna().any())\n",
        "metadata_training.dropna(subset=['episode_description', 'show_description'], inplace=True)\n",
        "print(\"\\nAfter dropping NaN values:\\n\", metadata_training.isna().any())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVR-08GLo_cB"
      },
      "source": [
        "#### Gold dataset of 150 episodes composed by 6 set of summaries for each episode (900 document-summary-grade triplets) that were graded on the Bad/Fair/Good/Excellent scale (0-3).\n",
        "\n",
        "We merge this gold dataset with the dataset we are going to clean, and the best summary of each episode will be considered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5bUjsepo4kI"
      },
      "outputs": [],
      "source": [
        "metadata_gold = pd.read_csv('/content/drive/MyDrive/682-project/data/150gold.tsv', sep='\\t')\n",
        "metadata_gold.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KroXH550pH1x"
      },
      "outputs": [],
      "source": [
        "# Bad = 1, Excellent = 4\n",
        "quality = {\n",
        "    'B': 1,\n",
        "    'F': 2,\n",
        "    'G': 3,\n",
        "    'E': 4\n",
        "}\n",
        "\n",
        "# convert egfb columns to a quality score\n",
        "egfb_columns = ['EGFB', 'EGFB.1', 'EGFB.2', 'EGFB.3', 'EGFB.4', 'EGFB.5']\n",
        "egfb_to_quality = metadata_gold[egfb_columns].applymap(lambda x: quality[x])\n",
        "\n",
        "# remove rows with no quality > 1\n",
        "egfb_to_quality = egfb_to_quality[[any(row > 1) for row in egfb_to_quality.values]]\n",
        "\n",
        "# select the best transcript for each episode\n",
        "best_egfb = egfb_to_quality.apply(lambda x: x.idxmax(), axis=1)\n",
        "best_summary = [metadata_gold.iloc[i, np.argwhere(metadata_gold.columns == egfb)[0][0] - 1] for i, egfb in best_egfb.iteritems()]\n",
        "\n",
        "metadata_gold = metadata_gold.loc[best_egfb.index]\n",
        "metadata_gold['best_summary'] = best_summary\n",
        "\n",
        "# create a dictionary of the best summary for each episode\n",
        "gold_summaries = {row['episode id']: row['best_summary'] for i, row in metadata_gold.iterrows()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTOwyTWvpQEw"
      },
      "outputs": [],
      "source": [
        "metadata_gold.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRkg79T-pS_M"
      },
      "outputs": [],
      "source": [
        "# substitute the episode descriptions correspondent to the episodes in the gold set with the best summary\n",
        "for i, row in metadata_training.iterrows():\n",
        "    if row['episode_uri'] in gold_summaries.keys():\n",
        "        metadata_training.at[i, 'episode_description'] = gold_summaries[row['episode_uri']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVZlCwJ7prXA"
      },
      "outputs": [],
      "source": [
        "metadata_training['episode_description'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xC5riVMqZ_n"
      },
      "source": [
        "#### We clean and improve these epsiode descriptions:\n",
        "\n",
        "1.   removing the content after \"---\" that usually is a sponsorship or a boilerplate (e.g., “--- This episode is sponsored by ...” “--- Send in a voice message”)\n",
        "2.   remove sentences that contain URLs, @mentions and email addresses in the episode descriptions\n",
        "3. remove tokens corresponding to emojii\n",
        "4. identify sentences that contain not useful content and remove them from the descriptions by computing a salience score for each sentence. This is done by summing over word IDF scores. Then we remove sentences if their salience scores are lower than a threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW2fRCEZqI5m"
      },
      "outputs": [],
      "source": [
        "def compute_document_frequencies(episode_descriptions):\n",
        "    \"\"\"\n",
        "    Compute the document frequencies in the whole dataset descriptions\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    episode_descriptions : list of str\n",
        "        The descriptions of the episodes\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A dictionary of word frequencies\n",
        "    \"\"\"\n",
        "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
        "\n",
        "    # get a set of words contained in each description (words are all lowercase)\n",
        "    flattened_descriptions = []\n",
        "    for description in tqdm(episode_descriptions, desc=\"Computing word frequencies\"):\n",
        "        description_set = set()\n",
        "        for sentence in seg.segment(description):\n",
        "            description_set.update([word.lower() for word in word_tokenize(sentence)])\n",
        "        flattened_descriptions.extend(list(description_set))\n",
        "\n",
        "    counts = pd.Series(Counter(flattened_descriptions))  # Get counts and transform to Series\n",
        "    return counts\n",
        "\n",
        "# compute the document frequencies that will be used to compute the sentence salience score\n",
        "document_frequencies = compute_document_frequencies(metadata_training['episode_description'])\n",
        "print(document_frequencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJYhVqQbrgf4"
      },
      "outputs": [],
      "source": [
        "# store the old dataframe to make comparisons\n",
        "metadata_train_old = metadata_training.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-2zOrBKvX_N"
      },
      "outputs": [],
      "source": [
        "def remove_boilerplate(description):\n",
        "    \"\"\"\n",
        "    Remove boilerplate from the episode description\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    description : str\n",
        "        The episode description\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A description without boilerplate (str)\n",
        "    \"\"\"\n",
        "    boilerplate_re = re.compile(r\"---.*\")\n",
        "    return boilerplate_re.sub(\"\", description)\n",
        "\n",
        "def remove_link_or_sponsors(description):\n",
        "    \"\"\"\n",
        "    Remove sentences containing links and sponsors or username and hashtag from the episode description\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    description : str\n",
        "        The episode description\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A description without links and sponsors (str)\n",
        "    \"\"\"\n",
        "    username_and_hashtag_re = re.compile(r\"(\\B@\\w+|\\B#\\w+)\")\n",
        "    links_or_sponsors_re = re.compile(\n",
        "        r\"(http|https|[pP]atreon|[eE]mail|[dD]onate|IG|[iI]nstagram|[fF]acebook|[yY]outube|[tT]witter|[dD]iscord|[fF]ollow|[sS]potify)\"\n",
        "    )\n",
        "\n",
        "    # remove username and hashtag\n",
        "    description = username_and_hashtag_re.sub(\" \", description)\n",
        "\n",
        "    # remove sentences containing links and sponsors\n",
        "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
        "    sentences = seg.segment(description)\n",
        "    sentences = [sentence for sentence in sentences if not links_or_sponsors_re.search(sentence)]\n",
        "    return \" \".join(sentences)\n",
        "\n",
        "def remove_emojii(description):\n",
        "    \"\"\"\n",
        "    Remove emojii from the episode description\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    description : str\n",
        "        The episode description\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A description without emojii (str)\n",
        "    \"\"\"\n",
        "    emoji_re = re.compile(r\"[^\\x00-\\x7F]+\")\n",
        "    return emoji_re.sub(\" \", description)\n",
        "\n",
        "print(\"\\nRemoving boilerplate from the episode descriptions:\")\n",
        "metadata_training['episode_description'] = metadata_training['episode_description'].progress_map(remove_boilerplate)\n",
        "\n",
        "print(\"Removing links and sponsors from the episode descriptions:\")\n",
        "metadata_training['episode_description'] = metadata_training['episode_description'].progress_map(remove_link_or_sponsors)\n",
        "\n",
        "print(\"Removing emojii from the episode descriptions:\")\n",
        "metadata_training['episode_description'] = metadata_training['episode_description'].progress_map(remove_emojii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy0gTWOAvg70"
      },
      "outputs": [],
      "source": [
        "# see a few examples of comparisons between the old and new descriptions\n",
        "samples = [137, 172]\n",
        "print(\"\\nExamples of comparisons before and after removing sponsors and links:\")\n",
        "for i in samples:\n",
        "        print(\"BEFORE:\"\n",
        "                f\"\\n\\t- {metadata_train_old['episode_description'].iloc[i]}\")\n",
        "        print(\"AFTER:\"\n",
        "                f\"\\n\\t- {metadata_training['episode_description'].iloc[i]}\")\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABxjvm0uyNJ2"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "def sentence_salience_score(sentence, num_descriptions, document_frequencies):\n",
        "    \"\"\"\n",
        "    Compute the salience score of a sentence by summing over word IDF scores.\n",
        "    Only alphabetic words that are longer than one character and are neither stop words nor words like 'episode' or 'podcast'\n",
        "    are considered when computing sentence salience scores.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sentence : str\n",
        "        The sentence to compute the salience score for\n",
        "    num_descriptions : int\n",
        "        The number of descriptions in the dataset\n",
        "    document_frequencies : pandas.Series\n",
        "        The document frequencies in the whole dataset descriptions\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    The salience score of the sentence (float)\n",
        "    \"\"\"\n",
        "    idf_scores = []\n",
        "    tokenized_sentence = word_tokenize(sentence)\n",
        "\n",
        "    # compute IDF scores for each word in the sentence and sum them up\n",
        "\n",
        "    for word in tokenized_sentence:\n",
        "        lower_world = word.lower()\n",
        "        # consider only alphabetic words, and remove stop words, single character\n",
        "        if lower_world in document_frequencies.keys() and lower_world.isalpha() and lower_world not in stopwords.words('english') and len(lower_world) > 1 and lower_world not in ['episode', 'podcast']:\n",
        "            # get document frequency\n",
        "            df = document_frequencies[lower_world]\n",
        "\n",
        "            # compute idf score\n",
        "            idf_score = np.log(num_descriptions/df)\n",
        "            idf_scores.append(idf_score)\n",
        "\n",
        "    idf_scores = np.array(idf_scores)\n",
        "    salience_score = idf_scores.mean() if len(idf_scores)>0 else 0.0\n",
        "    return salience_score\n",
        "\n",
        "def remove_unuseful_sentences(description, num_descriptions, word_frequencies, threshold=3.6):\n",
        "    \"\"\"\n",
        "    Remove sentences that are not useful for the transcriptions\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    description : str\n",
        "        The episode description\n",
        "    num_descriptions : int\n",
        "        The number of descriptions in the dataset\n",
        "    word_frequencies : pandas.Series\n",
        "        The word frequencies in the whole dataset descriptions\n",
        "    threshold : double\n",
        "        The threshold for the salience score of a sentence to be considered useful\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A description without unuseful sentences (str)\n",
        "    \"\"\"\n",
        "    # segment the text into sentences\n",
        "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
        "    sentences = seg.segment(description)\n",
        "    # remove sentences that are not useful for the transcriptions\n",
        "    sentences = [sentence for sentence in sentences if sentence_salience_score(sentence, num_descriptions, word_frequencies) > threshold]\n",
        "    return \" \".join(sentences)\n",
        "\n",
        "metadata_training['episode_description'] = metadata_training['episode_description'].progress_map(lambda x: remove_unuseful_sentences(x, metadata_training.shape[0], document_frequencies))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--2tuyCV1DUx"
      },
      "outputs": [],
      "source": [
        "# see a few examples of comparisons between the old and new descriptions\n",
        "samples = [137, 172]\n",
        "print(\"\\nExamples of comparisons before and after removing unuseful sentences:\")\n",
        "for i in samples:\n",
        "        print(\"BEFORE:\"\n",
        "                f\"\\n\\t- {metadata_train_old['episode_description'].iloc[i]}\")\n",
        "        print(\"AFTER:\"\n",
        "                f\"\\n\\t- {metadata_training['episode_description'].iloc[i]}\")\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw9X5FEF8l9f"
      },
      "outputs": [],
      "source": [
        "def check_length_brass(episode, upper_bound=750, lower_bound=20):\n",
        "    \"\"\"\n",
        "    Check if the episode descriptions is not too long (> 750 characters) or not too short (< 20 characters)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    episode : pandas.Series\n",
        "        A row from the metadata file\n",
        "    upper_bound : int\n",
        "        The upper bound of the episode description length\n",
        "    lower_bound : int\n",
        "        The lower bound of the episode description length\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Boolean indicating if the episode description is long enough\n",
        "    \"\"\"\n",
        "    return len(episode['episode_description']) <= upper_bound and len(episode['episode_description']) >= lower_bound\n",
        "\n",
        "def description_similarity(a, b):\n",
        "    \"\"\"\n",
        "    Measure the overlapping between two descriptions\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    a : str\n",
        "        The first description\n",
        "    b : str\n",
        "        The second description\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Value indicating the overlapping between the two descriptions\n",
        "    \"\"\"\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def check_show_description_overlap_brass(episode, thresh=0.5):\n",
        "    \"\"\"\n",
        "    Check if the episode descriptions overlapping with the show description is not too high (< 0.5)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    episode : pandas.Series\n",
        "        A row from the metadata file\n",
        "    thresh : float\n",
        "        The threshold of the overlap between the episode description and the show description\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Boolean indicating if the episode description is different enough from the show description\n",
        "    \"\"\"\n",
        "    return description_similarity(episode['show_description'], episode['episode_description']) < thresh\n",
        "\n",
        "def check_other_description_overlap_brass(episode, show_episodes, thresh=0.6):\n",
        "    \"\"\"\n",
        "    Check if the episode descriptions overlapping with the other description in the same show is not too high (< 0.6)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    episode : pandas.Series\n",
        "        A row from the metadata file\n",
        "    show_episodes : dict\n",
        "        A dictionary of the episodes of the same show\n",
        "    thresh : float\n",
        "        The threshold of the overlap between the episode description and the other description\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Boolean indicating if the episode description is different enough from the other description\n",
        "    \"\"\"\n",
        "    for other_prefix, other_description in show_episodes[episode['show_filename_prefix']]:\n",
        "        if other_prefix != episode['episode_filename_prefix'] and description_similarity(episode['episode_description'], other_description) > thresh and len(episode['episode_description']) < len(other_description):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "brass_set_lenght = metadata_training[metadata_training.progress_apply(check_length_brass, axis=1)]\n",
        "print(f\"Removed {len(metadata_training) - len(brass_set_lenght)} episodes ({(100-(len(brass_set_lenght)/len(metadata_training)*100)):.2f}%) because of too long or too short descriptions\")\n",
        "\n",
        "brass_set_show_overlap = brass_set_lenght[brass_set_lenght.progress_apply(check_show_description_overlap_brass, axis=1)]\n",
        "print(f\"Removed {len(brass_set_lenght) - len(brass_set_show_overlap)} episodes ({(100-(len(brass_set_show_overlap)/len(brass_set_lenght)*100)):.2f}%) because of too high overlap with the show description\")\n",
        "\n",
        "show_episodes = brass_set_show_overlap.groupby(['show_filename_prefix']).apply(lambda x: list(zip(x['episode_filename_prefix'], x['episode_description']))).to_dict()\n",
        "brass_set = brass_set_show_overlap[brass_set_show_overlap.progress_apply(lambda x: check_other_description_overlap_brass(x, show_episodes), axis=1)]\n",
        "print(f\"Removed {len(brass_set_show_overlap) - len(brass_set)} episodes ({(100-(len(brass_set)/len(brass_set_show_overlap)*100)):.2f}%) because of too high overlap with other descriptions in the same show\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Dh0VSgxBI8C"
      },
      "outputs": [],
      "source": [
        "# look to the removed episode descriptions due to the overlap with the show description\n",
        "removed_episodes_show_overlap = pd.concat([brass_set_lenght, brass_set_show_overlap]).drop_duplicates(keep=False)[['show_description', 'episode_description']]\n",
        "removed_episodes_show_overlap['overlapping'] = removed_episodes_show_overlap.apply(lambda row: description_similarity(row['show_description'], row['episode_description']), axis=1)\n",
        "\n",
        "num_to_visualize = 3\n",
        "\n",
        "for _ in range(num_to_visualize):\n",
        "    row = removed_episodes_show_overlap.sample()\n",
        "    print(f\"Episode description: \\n\\t{row['episode_description'].values[0]}\")\n",
        "    print(f\"Show description: \\n\\t{row['show_description'].values[0]}\")\n",
        "    print(f\"Overlapping score: \\n\\t{row['overlapping'].values[0]}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CatA3ACHUJX5"
      },
      "outputs": [],
      "source": [
        "# look to the removed episode descriptions due to the overlap with the other episode descriptions in the same show\n",
        "removed_episodes_other_overlap = pd.concat([brass_set, brass_set_show_overlap]).drop_duplicates(keep=False)[['show_filename_prefix', 'episode_filename_prefix', 'episode_description']]\n",
        "two_episodes_show  = {str(show_filename_prefix): show_episodes[show_filename_prefix] for show_filename_prefix in removed_episodes_other_overlap['show_filename_prefix'] if len(show_episodes[show_filename_prefix]) == 2 }\n",
        "removed_episodes_other_overlap = removed_episodes_other_overlap[removed_episodes_other_overlap['show_filename_prefix'].isin(two_episodes_show.keys())]\n",
        "other_episode_show = {}\n",
        "for i, row in removed_episodes_other_overlap.iterrows():\n",
        "    if row['show_filename_prefix'] in two_episodes_show:\n",
        "        if row['episode_filename_prefix'] in two_episodes_show[row['show_filename_prefix']][0]:\n",
        "            other_episode_show[row['show_filename_prefix']] = two_episodes_show[row['show_filename_prefix']][1][1]\n",
        "        else:\n",
        "            other_episode_show[row['show_filename_prefix']] = two_episodes_show[row['show_filename_prefix']][0][1]\n",
        "removed_episodes_other_overlap['other_episode_description'] = removed_episodes_other_overlap.apply(lambda row: other_episode_show[row['show_filename_prefix']], axis=1)\n",
        "removed_episodes_other_overlap['overlapping'] = removed_episodes_other_overlap.apply(lambda row: description_similarity(row['episode_description'], row['other_episode_description']), axis=1)\n",
        "\n",
        "num_to_visualize = 3\n",
        "\n",
        "for _ in range(num_to_visualize):\n",
        "    row = removed_episodes_other_overlap.sample()\n",
        "    print(f\"Episode description: \\n\\t{row['episode_description'].values[0]}\")\n",
        "    print(f\"Other episode description: \\n\\t{row['other_episode_description'].values[0]}\")\n",
        "    print(f\"Overlapping score: \\n\\t{row['overlapping'].values[0]}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c6yLWDXYCsN"
      },
      "source": [
        "#### Remove non english descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYYJmj8vVudY"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    nltk.data.find('corpora/words')\n",
        "except LookupError:\n",
        "    nltk.download('words')\n",
        "wordset = set(words.words())\n",
        "\n",
        "def is_english(text, threshold = 0.3):\n",
        "    \"\"\"\n",
        "    Check if the text is written in english\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to check\n",
        "    threshold : float\n",
        "        The threshold of the ratio of english words in the text\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Boolean indicating if the text is written in english\n",
        "    \"\"\"\n",
        "    tokenized = word_tokenize(text)\n",
        "    alpha_tokenized = [word.lower() for word in tokenized if word.isalpha()]\n",
        "    dictionary_score = sum([word.lower() in wordset for word in alpha_tokenized\n",
        "                           ]) / len(alpha_tokenized)\n",
        "    return dictionary_score > threshold\n",
        "\n",
        "# remove episodes with non english description\n",
        "len_old_brass_set = len(brass_set)\n",
        "brass_set = brass_set[brass_set.progress_apply(lambda x: is_english(x['episode_description']), axis=1)]\n",
        "print(f\"Removed {len_old_brass_set - len(brass_set)} episodes ({(100-(len(brass_set)/len_old_brass_set*100)):.2f}%) because of non english description\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV_e7Ro5X5-m"
      },
      "source": [
        "### BRASS SET IS READY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f08B-Q8VV8k4"
      },
      "outputs": [],
      "source": [
        "# store brass set\n",
        "brass_set.to_csv(os.path.join(os.path.dirname('/content/drive/MyDrive/682-project/data'), \"brass_set.tsv\"), index=False, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCw1bCsJW39Y"
      },
      "outputs": [],
      "source": [
        "brass_set_df = pd.read_csv('/content/drive/MyDrive/682-project/data/brass_set.tsv', delimiter='\\t')\n",
        "brass_set_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvkVmn-TYMl0"
      },
      "source": [
        "### Transcript filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1OGg88qe5kQ"
      },
      "outputs": [],
      "source": [
        "# load brass set\n",
        "brass_set = pd.read_csv(('/content/drive/MyDrive/682-project/data/brass_set.tsv'), sep='\\t')\n",
        "brass_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE-gHsKb4LNv"
      },
      "source": [
        "### Chunk classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asP94jrO4KvK"
      },
      "outputs": [],
      "source": [
        "# !pip install sentence-transformers\n",
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrP5AGBT4eWu"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from rouge import Rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1Vv-8nY4qyl"
      },
      "outputs": [],
      "source": [
        "# loading the dataset from the csv file\n",
        "cleaned_gold_file_path = '/content/drive/MyDrive/682-project/data/gold_set_cleaned.tsv'\n",
        "\n",
        "dataset = pd.read_csv(cleaned_gold_file_path, sep='\\t')\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVIohi_-43Ri"
      },
      "source": [
        "### Chunk Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHigNfDx41vJ"
      },
      "outputs": [],
      "source": [
        "def isChunkUseful(chunk, summary, metric, threshold, verbose=False):\n",
        "    \"\"\"\n",
        "    Function to check if a chunk is useful or not\n",
        "\n",
        "    Parameters:\n",
        "        - chunk: part of the transcript\n",
        "        - summary: summary of a transcript\n",
        "        - metric: function of ariety 2 (chunk, summary) used to evaluate the summary\n",
        "        - threshold: value used to decide whether chunk is a good summary or not\n",
        "    Returns:\n",
        "        - True if the chunk is a good summary, False otherwise\n",
        "    \"\"\"\n",
        "    score = metric(chunk, summary)\n",
        "    if verbose: print(f\"\\tChunck: {chunk}\\n\\tSummary: {summary}\\n\\tScore: {score}\")\n",
        "\n",
        "    if score < threshold:\n",
        "        result = False\n",
        "    else:\n",
        "        result = True\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R85zSIRl47dm"
      },
      "source": [
        "### ROUGE-L f1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dFcWaHn46GR"
      },
      "outputs": [],
      "source": [
        "def rouge_score(candidate, reference, type='rouge-l', metric='f'):\n",
        "    \"\"\"\n",
        "    ROUGE score\n",
        "    Parameters:\n",
        "        reference: reference text\n",
        "        candidate: candidate text\n",
        "        type: type of ROUGE, it can be rouge-1, rouge-2, rouge-l (default)\n",
        "        metric: precision (p), recall (r) or f-score (f) (default)\n",
        "    \"\"\"\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(candidate, reference)\n",
        "    return scores[0][type][metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAXaCW5Y4_E6"
      },
      "outputs": [],
      "source": [
        "threshold = 0.20\n",
        "metric = rouge_score\n",
        "verbose = False\n",
        "\n",
        "# creation of the dataset for chunk classification\n",
        "# creation of the targets\n",
        "\n",
        "features = []\n",
        "targets = []\n",
        "\n",
        "# initalize the model for the sentence transformer\n",
        "sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "for i in tqdm(range(len(dataset)), desc=\"Extracting features and targets\"):\n",
        "    if verbose: print(f\"Episode: {i}\")\n",
        "    chunks = semantic_segmentation(dataset.transcript[i], sentence_encoder)\n",
        "    description = dataset.best_summary[i]\n",
        "\n",
        "    num_chunks = len(chunks)\n",
        "    if verbose: print(f\"Num chunks: {num_chunks}\")\n",
        "\n",
        "    for j in range(num_chunks):\n",
        "        if verbose: print(f\"\\tChunk {j}\")\n",
        "        features.append(extract_features(chunks[j], sentence_encoder))\n",
        "        if isChunkUseful(' '.join(chunks[j]), description, metric, threshold, verbose):\n",
        "            targets.append(1)\n",
        "        else:\n",
        "            targets.append(0)\n",
        "\n",
        "y = np.array(targets)\n",
        "y = y.reshape(y.shape[0], 1)\n",
        "X = np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nve56sXM5uer"
      },
      "outputs": [],
      "source": [
        "print('y = ', y)\n",
        "print('y shape = ', len(y))\n",
        "print('X = ', X)\n",
        "print('X shape = ', len(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4X5QPpU-yko"
      },
      "outputs": [],
      "source": [
        "# show the percentage of useful and unuseful chunks\n",
        "positive = y[y==1].shape[0]\n",
        "negative = y.shape[0] - positive\n",
        "print(f\"Percentage of useful chunks: {positive/(positive+negative)*100}%\")\n",
        "print(f\"Percentage of unuseful chunks: {negative/(positive+negative)*100}%\")\n",
        "\n",
        "# store chunk classification dataset\n",
        "chunk_classification_dataset = np.hstack((X, y))\n",
        "df_chunk = pd.DataFrame(chunk_classification_dataset)\n",
        "df_chunk.to_csv(('/content/drive/MyDrive/682-project/data/chunk_classification_dataset.csv'), header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFoKPUpU_l2T"
      },
      "outputs": [],
      "source": [
        "chunk_classification_dataset = pd.read_csv('/content/drive/MyDrive/682-project/data/chunk_classification_dataset.csv')\n",
        "chunk_classification_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7dSefE3_7ip"
      },
      "outputs": [],
      "source": [
        "# The dataset contains 384 features and 1 target\n",
        "y = chunk_classification_dataset.iloc[:,-1]\n",
        "X = chunk_classification_dataset.drop(chunk_classification_dataset.columns[[-1]], axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAROYKwl__D5"
      },
      "outputs": [],
      "source": [
        "X_train_positive = X_train[y_train>0]\n",
        "X_train_negative = X_train[y_train==0][:X_train_positive.shape[0]]\n",
        "y_train_positive = y_train[y_train>0]\n",
        "y_train_negative = y_train[y_train==0][:X_train_positive.shape[0]]\n",
        "\n",
        "X_train = np.vstack((X_train_positive,X_train_negative))\n",
        "y_train = np.hstack((y_train_positive, y_train_negative))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSyreUwHAEnO"
      },
      "outputs": [],
      "source": [
        "# Neural Network for chunk classification\n",
        "\n",
        "inputs = keras.Input(shape=(384))\n",
        "x = keras.layers.Dense(512, activation='relu')(inputs)\n",
        "x = keras.layers.Dense(256, activation='relu')(x)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "x = keras.layers.Dense(256, activation='relu', kernel_regularizer='l2')(x)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "x = keras.layers.Dense(128, activation='relu', kernel_regularizer='l2')(x)\n",
        "output = keras.layers.Dense(1, activation='sigmoid', kernel_regularizer='l2')(x)\n",
        "model = keras.Model(inputs, output)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='Adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=16,\n",
        "    epochs=15,\n",
        "    validation_split=0.15,\n",
        "    validation_data=(X_test,y_test),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=3)]\n",
        ")\n",
        "\n",
        "# model.save(\"modelChunkNN\")\n",
        "model.save(\"/content/drive/MyDrive/682-project/modelChunkNN\")\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = [1 if y>0.5 else 0 for y in y_pred]\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average=None)}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average=None)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MWZNeS3W5My"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def get_path(episode, transcript_path):\n",
        "    \"\"\"\n",
        "    Get the path of the episode json file\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    episode : pandas.Series\n",
        "        A row from the metadata file\n",
        "    transcript_path : str\n",
        "        The absolute path of the folder containing the transcripts\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    path : str\n",
        "        The absolute path of the episode json file\n",
        "    \"\"\"\n",
        "    # extract the 2 reference number/letter to access the episode transcript\n",
        "    show_filename = episode['show_filename_prefix']\n",
        "    episode_filename = episode['episode_filename_prefix'] + \".json\"\n",
        "    dir_1, dir_2 = re.match(r'show_(\\d)(\\w).*', show_filename).groups()\n",
        "    transcipt_path = os.path.join(transcript_path, dir_2.upper(), show_filename, episode_filename)\n",
        "    print('show_filename = ', show_filename)\n",
        "    print('transcript_path = ', transcript_path)\n",
        "    return transcipt_path\n",
        "\n",
        "    # Only extract 0 and 1 folders\n",
        "\n",
        "#     match = re.match(r'show_([01])(\\w).*', show_filename)\n",
        "#     if match:\n",
        "#         # print('match = ', match)\n",
        "#         dir_1, dir_2 = match.groups()\n",
        "#         # check if the transcript file in all the derived subfolders exist\n",
        "\n",
        "#     else:\n",
        "#       raise FileNotFoundError(\"The show_filename does not match the expected pattern\")\n",
        "\n",
        "def get_transcription(episode):\n",
        "    \"\"\"\n",
        "    Extract the transcript from the episode json file\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    episode : pandas.Series\n",
        "        A row from the metadata file\n",
        "    dataset_path : str\n",
        "        The absolute path of the dataset\n",
        "    test_set : bool\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    transcript : str\n",
        "        The transcript of the episode\n",
        "    \"\"\"\n",
        "\n",
        "    transcript_path = os.path.join('E:/682-project/data/spotify-podcasts-2020/podcasts-transcripts/0')\n",
        "\n",
        "#     try:\n",
        "#       with open(get_path(episode, transcript_path), 'r') as f:\n",
        "#           print('f = ', f)\n",
        "#           episode_json = json.load(f)\n",
        "#           # seems that the last result in each trastcript is a repetition of the first one, so we ignore it\n",
        "#           transcripts = [\n",
        "#               result[\"alternatives\"][0]['transcript'] if 'transcript' in result[\"alternatives\"][0] else \"\"\n",
        "#               for result in episode_json[\"results\"][:-1]\n",
        "#           ]\n",
        "#           return \" \".join(transcripts)\n",
        "#     except FileNotFoundError:\n",
        "#         # Handle the case where the file is not found by returning an empty string\n",
        "#         return \"\"\n",
        "    try:\n",
        "        with open(get_path(episode, transcript_path), 'r') as f:\n",
        "            episode_json = json.load(f)\n",
        "            # Check if the 'results' key exists in the JSON\n",
        "            if 'results' in episode_json:\n",
        "                # Remove the last result if it's a repetition of the first one\n",
        "                results = episode_json.get('results', [])[:-1]\n",
        "                transcripts = [\n",
        "                    result[\"alternatives\"][0]['transcript'] if 'transcript' in result[\"alternatives\"][0] else \"\"\n",
        "                    for result in results\n",
        "                ]\n",
        "                return \" \".join(transcripts)\n",
        "            else:\n",
        "                # If 'results' key is not found, return an empty string\n",
        "                return \"\"\n",
        "    except FileNotFoundError:\n",
        "        # Handle the case where the file is not found by returning an empty string\n",
        "        return \"\"\n",
        "\n",
        "def look_ahead_chuck(sentences, lower_chunk_size):\n",
        "    \"\"\"\n",
        "    Look-ahead function to determine the next chunk\n",
        "    \"\"\"\n",
        "    if sum([len(s) for s in sentences]) < lower_chunk_size:\n",
        "        # if the remaining sentences size is smaller than the lower bound, we return the remaining sentences\n",
        "        return sentences\n",
        "    else:\n",
        "        # next chunk size should be at least the lower bound\n",
        "        for i in range(len(sentences)):\n",
        "            if sum([len(s) for s in sentences[:i+1]]) >= lower_chunk_size:\n",
        "                return sentences[:i+1]\n",
        "\n",
        "\n",
        "def semantic_segmentation(text, model, lower_chunk_size=300, upper_chunk_size=2000):\n",
        "    \"\"\"\n",
        "    Algorithm proposed by Moro et. al. (2022) to semantically segment long inputs into GPU memory-adaptable chunks.\n",
        "    https://www.aaai.org/AAAI22Papers/AAAI-3882.MoroG.pdf\n",
        "\n",
        "    Parameters\n",
        "    -------------\n",
        "    text: str\n",
        "        The text to be segmented\n",
        "    model: SentenceTransformer\n",
        "        The model to be used for the sentence embeddings\n",
        "    lower_chunk_size: int\n",
        "        The lower bound of the chunk size\n",
        "    upper_chunk_size: int\n",
        "        The upper bound of the chunk size\n",
        "    Return\n",
        "    -------\n",
        "    List of chunks of text\n",
        "    \"\"\"\n",
        "\n",
        "    # segment the text into sentences\n",
        "    if len(text) < 1: return\n",
        "#     print('text = ', text)\n",
        "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
        "    sentences = seg.segment(text)\n",
        "\n",
        "    # print('sentences = ', sentences)\n",
        "    chunks = []\n",
        "    current_chunk = [sentences[0]]\n",
        "#     print('current chunk = ', current_chunk)\n",
        "\n",
        "    # Iterate over the sentences in the text\n",
        "    for i, sentence in enumerate(sentences[1:]):\n",
        "        if sentence == sentences[-1]:\n",
        "            # If the sentence is the last one, we add it to the last chunk\n",
        "            current_chunk.append(sentence)\n",
        "            chunks.append(current_chunk)\n",
        "        elif sum([len(s) for s in current_chunk]) + len(sentence) < lower_chunk_size:\n",
        "            # standardize each chunk to a minimum size to best leverage the capability of Transformers\n",
        "            current_chunk.append(sentence)\n",
        "        elif sum([len(s) for s in current_chunk]) + len(sentence) > upper_chunk_size:\n",
        "            # if the chunk is too big, we add it to the list of chunks and start a new one\n",
        "            chunks.append(current_chunk)\n",
        "            current_chunk = [sentence]\n",
        "        else:\n",
        "            idx = i+1\n",
        "            next_chunk = look_ahead_chuck(sentences[idx+1:], lower_chunk_size)\n",
        "\n",
        "            # get the embedding of the previous chunk and the next chunk\n",
        "            current_embedding = model.encode(current_chunk)\n",
        "            next_embedding = model.encode(next_chunk)\n",
        "            sentence_embedding = model.encode([sentence])\n",
        "\n",
        "            # get the cosine similarity between the embedding of the embeddings\n",
        "            score_current_chunk = util.cos_sim(sentence_embedding, current_embedding).numpy().mean()\n",
        "            score_next_chunk = util.cos_sim(sentence_embedding, next_embedding).numpy().mean()\n",
        "\n",
        "            # if the score_current_chunk is higher than the score_next_chunk, we add the sentence to the current chunk\n",
        "            if score_current_chunk > score_next_chunk:\n",
        "                current_chunk.append(sentence)\n",
        "            else:\n",
        "                if sum([len(s) for s in current_chunk]) >= lower_chunk_size:\n",
        "                    chunks.append(current_chunk)\n",
        "                    current_chunk = [sentence]\n",
        "                else:\n",
        "                    current_chunk.append(sentence)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def extract_features(text, model):\n",
        "    \"\"\"\n",
        "    Extract features from text using the sentence transformer model which produce a vector of 384 dimensions for each sentence\n",
        "    From each chunk an encoding of each sentence is extracted using a pretrained RoBerta Transformer to obtain a dense encoding.\n",
        "    The encoding of the chunk is the mean of the encoding of its sentences.\n",
        "\n",
        "    Parameters:\n",
        "        - text: string representing a document\n",
        "        - model: sentence transformer model\n",
        "    Returns:\n",
        "        - extracted features\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    for sentence in text:\n",
        "        embeddings.append(model.encode(sentence))\n",
        "\n",
        "    features = np.mean(embeddings, axis=0)\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3nadYhLebM-"
      },
      "outputs": [],
      "source": [
        "brass_set = pd.read_csv(('E:/682-project/data/brass_set.tsv'), sep='\\t')\n",
        "brass_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cvtwk_uyWjk"
      },
      "outputs": [],
      "source": [
        "brass_set_minimized = brass_set.copy()\n",
        "brass_set_minimized.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KggD_UU8yWjl"
      },
      "outputs": [],
      "source": [
        "# Function to extract folder name and check if it contains '0'\n",
        "def check_for_folder_conditions(row):\n",
        "    folder_name_6th = row['show_filename_prefix'][5]  # Extract 6th character after 'show_'\n",
        "    folder_name_7th = row['show_filename_prefix'][6]  # Extract 7th character after 'show_'\n",
        "\n",
        "    return folder_name_6th == '0' and folder_name_7th in ['0', '1', '2', '3', '4']\n",
        "\n",
        "# Filter the dataframe to keep rows where contains_zero_folder is True\n",
        "brass_set_minimized = brass_set_minimized[brass_set_minimized.apply(check_for_folder_conditions, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4phpf-gyWjl"
      },
      "outputs": [],
      "source": [
        "brass_set_minimized.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuIakJX1yWjl"
      },
      "outputs": [],
      "source": [
        "num_rows = brass_set_minimized.shape[0]\n",
        "print(\"Number of rows:\", num_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkF5giKLyWjl"
      },
      "outputs": [],
      "source": [
        "brass_set_minimized.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6V45Mcsai9i"
      },
      "outputs": [],
      "source": [
        "def transcript_filtering(episode, chunk_classifier, sentence_encoder, tokenizer, test_set=False):\n",
        "    \"\"\"\n",
        "    Extract the most salient chunks inside the transcript of an episode\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    episode : pandas.Series\n",
        "        The episode to extract the chunks from\n",
        "    chunk_classifier : tf.Model\n",
        "        The classifier to use to extract the most salient chunks\n",
        "    sentence_encoder : tf.Model\n",
        "        The encoder to use to encode the sentences\n",
        "    tokenizer : AutoTokenizer\n",
        "        The BART tokenizer to use to tokenize the transcript\n",
        "    test_set : Boolean\n",
        "        If True, the trascriptions will be searched in the test set directory, otherwise in the training set directory (default: False)\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    Transcript after the selection of the most salient chunks\n",
        "    \"\"\"\n",
        "\n",
        "    # extraction of chunks from the episode\n",
        "    chunks = semantic_segmentation(get_transcription(episode), sentence_encoder)\n",
        "\n",
        "    # extraction of features for each chunk\n",
        "    if chunks is not None:\n",
        "      features = np.array([extract_features(chunk, sentence_encoder) for chunk in chunks])\n",
        "\n",
        "      # prediction of the classifier\n",
        "      y = chunk_classifier.predict(features)\n",
        "\n",
        "      # score for each chunk\n",
        "      scores = [{'idx': i, 'relevance':y[i]} for i in range(len(chunks))]\n",
        "\n",
        "      # sorting chunks according to the probability to be relevant\n",
        "      scores.sort(key=lambda e: e['relevance'], reverse=True)\n",
        "\n",
        "      # filter chunks according to a maximum amount of 1024 tokens\n",
        "      count = 0\n",
        "      i = 0\n",
        "      max_tokens = 1024\n",
        "      # until the number of tokens is not max_tokens and there are still chunks to tokenize\n",
        "      while count <= max_tokens and i < len(scores):\n",
        "          count += len(tokenizer(' '.join(chunks[scores[i]['idx']]))['input_ids'])\n",
        "          i += 1\n",
        "      # if total number of chunk is less than max_tokens\n",
        "      if i == len(scores):\n",
        "          relevant_chunks = [' '.join(chunk) for chunk in chunks]\n",
        "      # othewise if there are more token than max_tokens\n",
        "      else:\n",
        "          selected_chunks = {scores[j]['idx']: chunks[scores[j]['idx']] for j in range(i-1)}\n",
        "          # reoreder chunks in the original order\n",
        "          relevant_chunks = [' '.join(chunks[idx]) for idx in sorted(selected_chunks.keys())]\n",
        "\n",
        "      # return the new transcript\n",
        "      return ' '.join(relevant_chunks)\n",
        "    else: return None\n",
        "\n",
        "\n",
        "chunk_classifier = tf.keras.models.load_model(\"E:/682-project/model/modelChunkNN\")\n",
        "model_checkpoint = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "brass_set_minimized['filtered_transcript'] = brass_set_minimized.progress_apply(lambda x: transcript_filtering(x, chunk_classifier, sentence_encoder, tokenizer), axis=1)\n",
        "# transcripts = brass_set.progress_apply(lambda x: transcript_filtering(x, chunk_classifier, sentence_encoder, tokenizer), axis=1)\n",
        "\n",
        "brass_set_minimized[['episode_uri','filtered_transcript', 'episode_description']].to_csv((\"E:/682-project/data/filtered_set_minimized.csv\"), index=False)\n",
        "print(\"Filtering done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx0gju3ZRY4f"
      },
      "outputs": [],
      "source": [
        "# brass_set_minimized[['episode_uri','filtered_transcript', 'episode_description']].to_csv((\"E:/682-project/data/filtered_set_minimized.csv\"), index=False)\n",
        "null_counts = filtered_set_minimized.isna().sum()\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0d-fKsDpI2lp",
        "outputId": "61897308-646a-4c2b-a392-c7ad1a9f9fb1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode_uri</th>\n",
              "      <th>filtered_transcript</th>\n",
              "      <th>episode_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>spotify:episode:007I0vUfkdTg8FvE5WLFIl</td>\n",
              "      <td>This is the Premier League preview podcast fol...</td>\n",
              "      <td>It's Matchweek 25.   This week we preview the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spotify:episode:01X3QMRerbUz0YJ47atAAc</td>\n",
              "      <td>You date me you like you get to know me.  You ...</td>\n",
              "      <td>On this weeks Taste of Taylor, Taylor intervie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spotify:episode:01rlULldcOvtmXKi70zcco</td>\n",
              "      <td>My name is Giovanni, Georgia.  But everybody c...</td>\n",
              "      <td>In this episode, Giorgio Moroder joins David W...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>spotify:episode:02cmc8Kj2L214GakIX6s0w</td>\n",
              "      <td>Just wanted to take a brief moment to give you...</td>\n",
              "      <td>Your Host Dustin Nichols sits down with an old...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>spotify:episode:039r0XO3svhB04rQmDKwa8</td>\n",
              "      <td>Hello and welcome to the volunteer firefighter...</td>\n",
              "      <td>This Week we celebrate St Valentine's day as C...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              episode_uri  \\\n",
              "0  spotify:episode:007I0vUfkdTg8FvE5WLFIl   \n",
              "1  spotify:episode:01X3QMRerbUz0YJ47atAAc   \n",
              "2  spotify:episode:01rlULldcOvtmXKi70zcco   \n",
              "3  spotify:episode:02cmc8Kj2L214GakIX6s0w   \n",
              "4  spotify:episode:039r0XO3svhB04rQmDKwa8   \n",
              "\n",
              "                                 filtered_transcript  \\\n",
              "0  This is the Premier League preview podcast fol...   \n",
              "1  You date me you like you get to know me.  You ...   \n",
              "2  My name is Giovanni, Georgia.  But everybody c...   \n",
              "3  Just wanted to take a brief moment to give you...   \n",
              "4  Hello and welcome to the volunteer firefighter...   \n",
              "\n",
              "                                 episode_description  \n",
              "0  It's Matchweek 25.   This week we preview the ...  \n",
              "1  On this weeks Taste of Taylor, Taylor intervie...  \n",
              "2  In this episode, Giorgio Moroder joins David W...  \n",
              "3  Your Host Dustin Nichols sits down with an old...  \n",
              "4  This Week we celebrate St Valentine's day as C...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_set_minimized = pd.read_csv(\"E:/682-project/data/filtered_set_minimized.csv\")\n",
        "filtered_set_minimized.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyuN64URyWjm"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjXCwtWoyqIq",
        "outputId": "94f0ae54-cfc9-4a81-818e-c88d28006126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 693\n",
            "Validation set size: 77\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset('csv', data_files=(\"E:/682-project/data/filtered_set_minimized.csv\"))\n",
        "\n",
        "train_set, validation_set = dataset['train'].train_test_split(test_size=0.1).values()\n",
        "print(f\"Training set size: {train_set.num_rows}\")\n",
        "print(f\"Validation set size: {validation_set.num_rows}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzsgy7dhyWjm"
      },
      "outputs": [],
      "source": [
        "# Check train_set\n",
        "# print(type(train_set))\n",
        "for i, sample in enumerate(train_set):\n",
        "    if i>5: break\n",
        "    print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpB9rdyXdYb5"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "\n",
        "metric = load(\"rouge\")\n",
        "# !pip install rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOB99NeXyWjm"
      },
      "source": [
        "### Try model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgF8zyahyWjm",
        "outputId": "65c9db40-1b1f-4d27-ffdd-e8fe44efce0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "Step 0/346, Loss: 10.5663\n"
          ]
        }
      ],
      "source": [
        "from transformers import BartTokenizer, TFBartForConditionalGeneration\n",
        "\n",
        "# Initialize the BART model and tokenizer\n",
        "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = BartTokenizer.from_pretrained(pretrained_model_name)\n",
        "model = TFBartForConditionalGeneration.from_pretrained(pretrained_model_name)\n",
        "\n",
        "# Prepare the data\n",
        "input_texts = [item['filtered_transcript'] for item in train_set]\n",
        "target_texts = [item['episode_description'] for item in train_set]\n",
        "\n",
        "# Tokenize and encode the input and target sequences\n",
        "inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "targets = tokenizer(target_texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "dataset = tf.data.Dataset.from_tensor_slices((dict(inputs), dict(targets)))\n",
        "train_dataset = dataset.shuffle(len(train_set)).batch(2)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for step, batch in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            input_ids = batch[0]['input_ids']\n",
        "            decoder_input_ids = batch[1]['input_ids']\n",
        "            labels = batch[1]['input_ids']  # Same as decoder input for Bart\n",
        "\n",
        "            logits = model(input_ids, decoder_input_ids=decoder_input_ids, return_dict=True).logits\n",
        "            loss = loss_fn(labels, logits)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}/{len(train_set)//2}, Loss: {loss:.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKiGS5KqyWjn"
      },
      "source": [
        "### Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP-ZZ49ZHavi"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpd4EVngH3iV"
      },
      "outputs": [],
      "source": [
        "def batch_tokenize_preprocess(dataset, text_column, summary_column, tokenizer, max_input_length, max_target_length):\n",
        "    \"\"\"\n",
        "    Preprocess a dataset by tokenizing the transcript and the summary.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : Dataset\n",
        "        The dataset to preprocess\n",
        "    text_column : str\n",
        "        The name of the column containing the transcript\n",
        "    summary_column : str\n",
        "        The name of the column containing the summary\n",
        "    tokenizer : AutoTokenizer\n",
        "        The tokenizer to use\n",
        "    max_input_length : int\n",
        "        The maximum length of the input sequence\n",
        "    max_target_length : int\n",
        "        The maximum length of the target sequence\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    The preprocessed dataset\n",
        "    \"\"\"\n",
        "    inputs = dataset[text_column]\n",
        "    targets = dataset[summary_column]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGpvlDvNyWjn"
      },
      "outputs": [],
      "source": [
        "max_input_length = 1024\n",
        "max_target_length = 256\n",
        "\n",
        "\n",
        "train_set_tokenized = train_set.map(\n",
        "    lambda batch: batch_tokenize_preprocess(\n",
        "        batch, \"filtered_transcript\", \"episode_description\", tokenizer, max_input_length, max_target_length\n",
        "    ),\n",
        "    batched=True,\n",
        "    remove_columns=train_set.column_names,\n",
        "    desc=\"Running tokenizer on train dataset\"\n",
        ")\n",
        "\n",
        "validation_set_tokenized = validation_set.map(\n",
        "    lambda batch: batch_tokenize_preprocess(\n",
        "        batch, \"filtered_transcript\", \"episode_description\", tokenizer, max_input_length, max_target_length\n",
        "    ),\n",
        "    batched=True,\n",
        "    remove_columns=validation_set.column_names,\n",
        "    desc=\"Running tokenizer on validation dataset\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMd8ys6byWjn"
      },
      "outputs": [],
      "source": [
        "# for i, sample in enumerate(train_set_tokenized):\n",
        "#     if i>5: break\n",
        "#     print(sample)\n",
        "\n",
        "train_set_tokenized_df = train_set_tokenized.to_pandas()\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "train_set_tokenized_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bQVilbbyWjn"
      },
      "outputs": [],
      "source": [
        "train_set_tokenized_df.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH68Z_3gyWjn"
      },
      "outputs": [],
      "source": [
        "train_set_tokenized_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrfsImgyyWjn"
      },
      "outputs": [],
      "source": [
        "validation_set_tokenized_df = validation_set_tokenized.to_pandas()\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "validation_set_tokenized_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncIYjZLAyWjn"
      },
      "outputs": [],
      "source": [
        "validation_set_tokenized_df.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQmZ6CuVyWjn"
      },
      "outputs": [],
      "source": [
        "validation_set_tokenized_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AJbeBrUyWjn"
      },
      "source": [
        "### Fine-tuning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQ_ef1YuyWjn"
      },
      "outputs": [],
      "source": [
        "# prepare the model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ_lGLmuyWjn"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxrHWF5oyWjo"
      },
      "outputs": [],
      "source": [
        "# parameters for the training\n",
        "batch_size = 2\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "num_train_epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdh4_fj0yWjo"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_set_tokenized.to_tf_dataset(\n",
        "    batch_size=batch_size,\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "validation_dataset = validation_set_tokenized.to_tf_dataset(\n",
        "    batch_size=batch_size,\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "generation_dataset = (\n",
        "    validation_set_tokenized\n",
        "    .shuffle()\n",
        "    .select(range(min(200, len(validation_set_tokenized))))\n",
        "    .to_tf_dataset(\n",
        "        batch_size=batch_size,\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "        shuffle=False,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2emRoTdyWjo"
      },
      "outputs": [],
      "source": [
        "for i, sample in enumerate(train_dataset):\n",
        "    if i>5: break\n",
        "    print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmi-DfMvyWjo"
      },
      "outputs": [],
      "source": [
        "# optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-6)\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS1RUbRNyWjo"
      },
      "outputs": [],
      "source": [
        "def metric_fn(eval_predictions):\n",
        "    predictions, labels = eval_predictions\n",
        "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    for label in labels:\n",
        "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_predictions = [\n",
        "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
        "    ]\n",
        "    decoded_labels = [\n",
        "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
        "    ]\n",
        "    result = metric.compute(\n",
        "        predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
        "    )\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    # Add mean generated length\n",
        "    prediction_lens = [\n",
        "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
        "    ]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "model_path = \"E:/682-project/model/bart-large-finetuned/filtered-spotify-podcast-summ\"\n",
        "\n",
        "log_dir = model_path + \"/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True\n",
        ")\n",
        "\n",
        "callbacks = [metric_callback, tensorboard_callback]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymf7bgthyWjo"
      },
      "outputs": [],
      "source": [
        "# Assuming you have tokenized inputs stored in `model_inputs[\"input_ids\"]`\n",
        "\n",
        "# Get token IDs\n",
        "token_ids = train_set_tokenized[\"input_ids\"]\n",
        "\n",
        "# Check the maximum and minimum token IDs\n",
        "max_token_id = max([max(seq) for seq in token_ids])\n",
        "min_token_id = min([min(seq) for seq in token_ids])\n",
        "\n",
        "# Get the vocabulary size of the tokenizer\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "# Verify the token IDs range\n",
        "print(f\"Maximum Token ID: {max_token_id}\")\n",
        "print(f\"Minimum Token ID: {min_token_id}\")\n",
        "print(f\"Tokenizer Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Check if the token IDs fall within the expected range\n",
        "if min_token_id >= 0 and max_token_id < vocab_size:\n",
        "    print(\"Token IDs fall within the tokenizer's vocabulary range.\")\n",
        "else:\n",
        "    print(\"Token IDs fall outside the tokenizer's vocabulary range.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHTo0OEfyWjo"
      },
      "outputs": [],
      "source": [
        "# fine-tune the model\n",
        "history = model.fit(\n",
        "    train_dataset, validation_data=validation_dataset, epochs=num_train_epochs, callbacks=callbacks\n",
        ")\n",
        "history = history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ee4JNUWyWjo"
      },
      "outputs": [],
      "source": [
        "hub_model_id = \"bart-large-finetuned-filtered-spotify-podcast-summ\"\n",
        "model.push_to_hub(hub_model_id)\n",
        "tokenizer.push_to_hub(hub_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p2H8KZ0yWjo"
      },
      "source": [
        "### History of the fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qVpCEsIyWjo"
      },
      "outputs": [],
      "source": [
        "# convert the history.history dict to a pandas DataFrame:\n",
        "path_model_history = os.path.join(model_path, 'history')\n",
        "if not os.path.exists(path_model_history):\n",
        "    os.makedirs(path_model_history)\n",
        "\n",
        "df_history = pd.DataFrame(history)\n",
        "with open(os.path.join(path_model_history, \"history.csv\"), mode=\"w\") as file:\n",
        "    df_history.to_csv(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ncXtxYcyWjo"
      },
      "outputs": [],
      "source": [
        "# Restore history\n",
        "cols = ['loss','val_loss','rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'gen_len']\n",
        "path_model_history = os.path.join(model_path, 'history')\n",
        "history = pd.read_csv(os.path.join(path_model_history, \"history.csv\"), usecols=cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g55szz44yWjp"
      },
      "outputs": [],
      "source": [
        "def plot_history(models_history, keys, model_names=[], labels=(\"epochs\", \"metrics\"), y_scale=\"linear\", figsize=(10,5), cmap='rainbow'):\n",
        "    \"\"\"\n",
        "    Plot the history of the metrics in the history dictionary for each model.\n",
        "        :param models_history: array of dictionary of the metric history for each model\n",
        "        :param keys: list of keys of the metrics to plot\n",
        "        :param model_names: list of names of the models\n",
        "        :param labels: list of labels of the axes\n",
        "        :param figsize: size of the figure\n",
        "        :param cmap: color map used for the plot\n",
        "    \"\"\"\n",
        "\n",
        "    # maps each model to a distinct RGB color\n",
        "    cmap = plt.cm.get_cmap(cmap, len(keys))\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "\n",
        "    # for each model trained\n",
        "    for i, history in enumerate(models_history):\n",
        "        # take all pairs of training and val metrics\n",
        "        for j, metric in enumerate(keys):\n",
        "            plt.plot(history[metric], label=f\"{model_names[i]} {metric}\", linestyle=\"solid\", color=cmap(j))\n",
        "\n",
        "    plt.xlabel(labels[0])\n",
        "    plt.ylabel(labels[1])\n",
        "    plt.yscale(y_scale)\n",
        "\n",
        "    # Adding legend\n",
        "    plt.legend(\n",
        "          title =\"Legend\",\n",
        "          loc =\"best\",\n",
        "          bbox_to_anchor=(1, 0.5))\n",
        "    plt.title(\"Training history\")\n",
        "    plt.grid(linestyle='--', linewidth=1)\n",
        "    plt.show()\n",
        "\n",
        "model_history = [history]\n",
        "model_names = [\"BART fine-tuning\"]\n",
        "plot_history(model_history, keys=['loss', 'val_loss'], model_names=model_names, labels=(\"epochs\", \"loss\"), figsize=(6,3))\n",
        "plot_history(model_history, keys=['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], model_names=model_names, labels=(\"epochs\", \"rouge\"), figsize=(6,3), cmap=\"cool\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFQ-FSHoyWjp"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHCdz6wlyWjp"
      },
      "outputs": [],
      "source": [
        "metadata_path_test = os.path.join(dataset_path, \"spotify-podcasts-2020\", \"metadata-summarization-testset.tsv\")\n",
        "metadata_test = pd.read_csv(metadata_path_test, sep='\\t')\n",
        "print(\"Columns: \", metadata_test.columns)\n",
        "print(\"Shape: \", metadata_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UUcpOgvyWjp"
      },
      "outputs": [],
      "source": [
        "# drop NaN values or empty descriptions if any\n",
        "metadata_test.dropna(subset=['episode_description', 'show_description'], inplace=True)\n",
        "metadata_test = metadata_test[[len(desc.strip()) > 0 for desc in metadata_test['episode_description']]]\n",
        "print(\"Test set size after dropping NaN values: \\n\", metadata_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS4MzmYEyWjp"
      },
      "outputs": [],
      "source": [
        "# load the tokenizer from the hub\n",
        "model_finetuned_checkpoint = \"gmurro/bart-large-finetuned-filtered-spotify-podcast-summ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_finetuned_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuTgkjJoyWjp"
      },
      "source": [
        "### Transcript filtering for the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in7khJx_yWjp"
      },
      "outputs": [],
      "source": [
        "chunk_classifier = keras.models.load_model(\"modelChunkNN\")\n",
        "sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "metadata_test['filtered_transcript'] = metadata_test.progress_apply(lambda x: transcript_filtering(x, chunk_classifier, sentence_encoder, tokenizer, test_set=True), axis=1)\n",
        "\n",
        "metadata_test[['episode_uri','filtered_transcript', 'episode_description']].to_csv(os.path.join(dataset_path, \"filtered_testset.csv\"), index=False)\n",
        "print(\"Filtering done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-cSgY1EyWjp"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vVYfGvDyWjp"
      },
      "outputs": [],
      "source": [
        "# load the filtered test set\n",
        "test_set = load_dataset('csv', data_files=os.path.join(dataset_path, \"filtered_testset.csv\"))['train']\n",
        "print(f\"Test set size: {test_set.num_rows}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU45n2TXyWjp"
      },
      "outputs": [],
      "source": [
        "def dataset_to_tf(dataset, tokenizer, model, max_input_length = 1024, max_target_length = 256, eval_batch_size = 2):\n",
        "    \"\"\"\n",
        "    Convert a dataset to a tensorflow dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : Dataset\n",
        "        The dataset to convert.\n",
        "    tokenizer : AutoTokenizer\n",
        "        The tokenizer to use.\n",
        "    model : TFAutoModelForSeq2SeqLM\n",
        "        The model to use.\n",
        "    max_input_length : int\n",
        "        The maximum length of the input sequences. Default: 1024.\n",
        "    max_target_length : int\n",
        "        The maximum length of the target sequences. Default: 256.\n",
        "    eval_batch_size : int\n",
        "        The batch size used for evaluation. Default: 2.\n",
        "    \"\"\"\n",
        "\n",
        "    # tokenize the set\n",
        "    set_tokenized = dataset.map(\n",
        "        lambda batch: batch_tokenize_preprocess(\n",
        "            batch, \"filtered_transcript\", \"episode_description\", tokenizer, max_input_length, max_target_length\n",
        "        ),\n",
        "        batched=True,\n",
        "        remove_columns=dataset.column_names,\n",
        "        desc=\"Running tokenizer on the given dataset\"\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
        "\n",
        "    dataset_tf  = set_tokenized.to_tf_dataset(\n",
        "        batch_size=eval_batch_size,\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "        shuffle=False,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "    return dataset_tf\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "\n",
        "    # rougeLSum expects newline after each sentence\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def predict(test_dataset, model, tokenizer, gen_kwargs, eval_batch_size):\n",
        "    \"\"\"\n",
        "    Generate predictions for the test set\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    test_dataset : datasets.Dataset\n",
        "        Test set\n",
        "    model : tf.keras.Model\n",
        "        Model to use for generation\n",
        "    tokenizer : transformers.AutoTokenizer\n",
        "        Tokenizer to use for generation\n",
        "    gen_kwargs : dict\n",
        "        Keyword arguments for the generation\n",
        "    eval_batch_size : int\n",
        "        Batch size for evaluation\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Pair correspoding to the list of predictions and the list of labels\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    references  = []\n",
        "\n",
        "    # convert the dataset to a tensorflow dataset prebatched\n",
        "    testset_tf = dataset_to_tf(test_dataset, tokenizer, model, max_input_length = 1024, max_target_length = 256, eval_batch_size = eval_batch_size)\n",
        "\n",
        "    # generate the predicted summaries\n",
        "    for batch in tqdm(testset_tf, desc=\"Generating summaries\", total=len(test_dataset)//eval_batch_size):\n",
        "        labels = batch.pop(\"labels\")\n",
        "        batch.update(gen_kwargs)\n",
        "        generated_tokens = model.generate(**batch)\n",
        "        if isinstance(generated_tokens, tuple):\n",
        "            generated_tokens = generated_tokens[0]\n",
        "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "        predictions.extend(decoded_preds)\n",
        "        references.extend(decoded_labels)\n",
        "    return predictions, references"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyENq7-NyWjp"
      },
      "outputs": [],
      "source": [
        "model_checkpoint_finetuned = \"gmurro/bart-large-finetuned-filtered-spotify-podcast-summ\"\n",
        "tokenizer_finetuned = AutoTokenizer.from_pretrained(model_checkpoint_finetuned)\n",
        "model_finetuned = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint_finetuned)\n",
        "\n",
        "model_checkpoint_pretrained = \"facebook/bart-large-cnn\"\n",
        "tokenizer_pretrained = AutoTokenizer.from_pretrained(model_checkpoint_pretrained)\n",
        "model_pretrained = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint_pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm3LvE-ZyWjq"
      },
      "outputs": [],
      "source": [
        "# bart generation parameters\n",
        "gen_kwargs = {\n",
        "    \"length_penalty\": 2.0,\n",
        "    \"num_beams\": 4,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"min_length\": 39,\n",
        "    \"max_length\": 250\n",
        "    }\n",
        "\n",
        "# predict on the test set with the finetuned model\n",
        "predictions_ft, references_ft = predict(test_set, model_finetuned, tokenizer_finetuned, gen_kwargs, eval_batch_size=2)\n",
        "\n",
        "# predict on the test set with the pretrained model\n",
        "predictions_pt, references_pt = predict(test_set, model_pretrained, tokenizer_pretrained, gen_kwargs, eval_batch_size=1)\n",
        "print(\"Predictions done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2guqR_hGyWjq"
      },
      "outputs": [],
      "source": [
        "references_ft[10].replace(\"\\n\", \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceO_n42IyWjq"
      },
      "outputs": [],
      "source": [
        "num_to_visualize = 3\n",
        "\n",
        "for _ in range(num_to_visualize):\n",
        "    i = np.random.randint(0, len(references_ft))\n",
        "\n",
        "    ref = references_ft[i].replace('\\n', ' ')\n",
        "    ft = predictions_ft[i].replace('\\n', ' ')\n",
        "    pt = predictions_pt[i].replace('\\n', ' ')\n",
        "    print(f\"Creator-provided description: \\n\\t{ref}\")\n",
        "    print(f\"Fine-tuned model prediction: \\n\\t{ft}\")\n",
        "    print(f\"Pre-trained model prediction: \\n\\t{pt}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehdiR5TMyWjq"
      },
      "outputs": [],
      "source": [
        "# read predictions and references (if stored)\n",
        "df_pred = pd.read_csv(os.path.join(dataset_path, \"predictions_ft.csv\"))\n",
        "predictions_ft = df_pred['predictions'].tolist()\n",
        "references_ft = df_pred['references'].tolist()\n",
        "\n",
        "df_pred = pd.read_csv(os.path.join(dataset_path, \"predictions_pt.csv\"))\n",
        "predictions_pt = df_pred['predictions'].tolist()\n",
        "references_pt = df_pred['references'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiB_hnpLyWjq"
      },
      "outputs": [],
      "source": [
        "def rouge_evaluation(predictions, references):\n",
        "    \"\"\"\n",
        "    Evaluate the ROUGE score for the given predictions and references\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    predictions : list\n",
        "        List of predictions\n",
        "    references : list\n",
        "        List of references\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        ROUGE score\n",
        "    \"\"\"\n",
        "    rouge = evaluate.load('rouge')\n",
        "    results_rouge = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "\n",
        "    df_rouge = pd.DataFrame({'precision': [round(value.mid.precision,4) for key, value in results_rouge.items()],\n",
        "                             'recall': [round(value.mid.recall,4) for key, value in results_rouge.items()],\n",
        "                             'f1': [round(value.mid.fmeasure,4) for key, value in results_rouge.items()]},\n",
        "                             index=results_rouge.keys())\n",
        "\n",
        "    return df_rouge\n",
        "\n",
        "def bertscore_evaluation(predictions, references, idf_weighting=True):\n",
        "    \"\"\"\n",
        "    Evaluate the BERTScore score for the given predictions and references\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    predictions : list\n",
        "        List of predictions\n",
        "    references : list\n",
        "        List of references\n",
        "    idf_weighting : bool\n",
        "        Whether to use idf weighting\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        BERTScore score\n",
        "    \"\"\"\n",
        "    precision, recall, fmeasure = bert_score.score(cands=predictions, refs=references, lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\", num_layers=40, idf=idf_weighting)\n",
        "    df_bertscore = pd.DataFrame({'precision': [round(precision.mean().item(), 4), round(precision.std().item(), 4)],\n",
        "                                 'recall': [round(recall.mean().item(), 4), round(recall.std().item(),4)],\n",
        "                                 'f1': [round(fmeasure.mean().item(), 4), round(fmeasure.std().item(),4)]},\n",
        "                             index=[\"mean\", \"std\"])\n",
        "\n",
        "    return df_bertscore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IShri0KcyWjq"
      },
      "source": [
        "### Evaluation of the fine-tuned model bart-large-finetuned-filtered-spotify-podcast-summ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yLTL9ZryWjq"
      },
      "outputs": [],
      "source": [
        "# compute the ROUGE score on fine-tuned model\n",
        "rouge_ft = rouge_evaluation(predictions_ft, references_ft)\n",
        "rouge_ft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gAfofTNyWjq"
      },
      "outputs": [],
      "source": [
        "# compute the BERT score with IDF on fine-tuned model\n",
        "bertscore_ft = bertscore_evaluation(predictions_ft, references_ft, idf_weighting=True)\n",
        "bertscore_ft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1X12E2HyWjq"
      },
      "outputs": [],
      "source": [
        "# compute the BERT score without IDF on fine-tuned model\n",
        "bertscore_ft = bertscore_evaluation(predictions_ft, references_ft, idf_weighting=False)\n",
        "bertscore_ft"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}